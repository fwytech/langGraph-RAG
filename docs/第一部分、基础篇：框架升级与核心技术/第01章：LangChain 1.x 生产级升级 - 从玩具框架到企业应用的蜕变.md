# ç¬¬01ç« ï¼šLangChain 1.x ç”Ÿäº§çº§å‡çº§ - ä»ç©å…·æ¡†æ¶åˆ°ä¼ä¸šåº”ç”¨çš„èœ•å˜

> **æœ¬ç« ç›®æ ‡**ï¼š
> 1. ç†è§£ LangChain 1.x ä¸ 0.x çš„æ ¸å¿ƒå·®å¼‚ï¼ˆåŒ…ç»“æ„ã€API è®¾è®¡ã€ä¾èµ–ç®¡ç†ï¼‰
> 2. æŒæ¡ LangChain 1.x çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆChatModelã€Messagesã€Runnableï¼‰
> 3. å¿«é€Ÿæ­å»ºç¬¬ä¸€ä¸ª LangChain 1.x å¯¹è¯åº”ç”¨ï¼ˆ5åˆ†é’Ÿå®æˆ˜ï¼‰
> 4. å­¦ä¼šä» 0.x å¹³æ»‘è¿ç§»åˆ° 1.x çš„æœ€ä½³å®è·µ

---

## ä¸€ã€ä¸ºä»€ä¹ˆ LangChain éœ€è¦ 1.x é‡æ„ï¼Ÿ

### 1.1 LangChain 0.x çš„å†å²é—®é¢˜

LangChain 0.x åœ¨ 2022-2024 å¹´é—´è¿…é€Ÿæµè¡Œï¼Œæˆä¸ºæ„å»º AI åº”ç”¨çš„çƒ­é—¨æ¡†æ¶ã€‚ä½†éšç€é¡¹ç›®è§„æ¨¡æ‰©å¤§ï¼Œå¼€å‘è€…é€æ¸å‘ç°äº†å‡ ä¸ªè‡´å‘½é—®é¢˜ï¼š

**é—®é¢˜1ï¼šåŒ…ç»“æ„æ··ä¹±ï¼Œä¾èµ–è‡ƒè‚¿**

```python
# 0.x æ—¶ä»£ï¼šä¸€ä¸ªåŒ…æ‰“å¤©ä¸‹
pip install langchain  # å®‰è£…ååŒ…å«æ‰€æœ‰é›†æˆï¼Œä½“ç§¯è¶…è¿‡ 500MB

from langchain.llms import OpenAI          # OpenAI é›†æˆ
from langchain.llms import HuggingFace     # HuggingFace é›†æˆ
from langchain.vectorstores import Chroma  # Chroma é›†æˆ
from langchain.vectorstores import Pinecone # Pinecone é›†æˆ
# ... è¿˜æœ‰å‡ åä¸ªå…¶ä»–é›†æˆï¼Œå…¨éƒ¨æ‰“åŒ…åœ¨ä¸€èµ·
```

**ç—›ç‚¹**ï¼š
- âŒ åªæƒ³ç”¨ OpenAIï¼Œå´è¦å®‰è£…æ‰€æœ‰é›†æˆçš„ä¾èµ–
- âŒ éƒ¨ç½²ç¯å¢ƒä½“ç§¯è¿‡å¤§ï¼ˆDocker é•œåƒåŠ¨è¾„å‡ ä¸ª GBï¼‰
- âŒ ä¾èµ–å†²çªé¢‘ç¹ï¼ˆä¸åŒé›†æˆè¦æ±‚çš„åº“ç‰ˆæœ¬ä¸åŒï¼‰

**é—®é¢˜2ï¼šAPI è®¾è®¡ä¸ç»Ÿä¸€ï¼Œå­¦ä¹ æ›²çº¿é™¡å³­**

```python
# 0.x æ—¶ä»£ï¼šä¸åŒæ¨¡å‹çš„è°ƒç”¨æ–¹å¼å„ä¸ç›¸åŒ
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# è°ƒç”¨ LLM
llm = OpenAI()
result1 = llm("é—®é¢˜")           # è¿”å›å­—ç¬¦ä¸²

# è°ƒç”¨ ChatModel
chat = ChatOpenAI()
result2 = chat.predict("é—®é¢˜")  # ä½¿ç”¨ predict æ–¹æ³•
result3 = chat([HumanMessage(content="é—®é¢˜")])  # ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨
```

**ç—›ç‚¹**ï¼š
- âŒ åŒæ ·æ˜¯è°ƒç”¨æ¨¡å‹ï¼Œæ–¹æ³•åä¸ç»Ÿä¸€ï¼ˆ`__call__` vs `predict` vs `invoke`ï¼‰
- âŒ è¿”å›æ ¼å¼ä¸ä¸€è‡´ï¼ˆå­—ç¬¦ä¸² vs æ¶ˆæ¯å¯¹è±¡ï¼‰
- âŒ æ¨¡å‹åˆ‡æ¢éœ€è¦ä¿®æ”¹å¤§é‡ä»£ç 

**é—®é¢˜3ï¼šç±»å‹å®‰å…¨ç¼ºå¤±ï¼Œè°ƒè¯•å›°éš¾**

```python
# 0.x æ—¶ä»£ï¼šç±»å‹æç¤ºä¸å®Œå–„
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(input_data)  # ä¸çŸ¥é“è¿”å›ä»€ä¹ˆç±»å‹
# IDE æ— æ³•æä¾›æœ‰æ•ˆçš„ä»£ç è¡¥å…¨å’Œé”™è¯¯æ£€æŸ¥
```

**ç—›ç‚¹**ï¼š
- âŒ æ²¡æœ‰ä¸¥æ ¼çš„ç±»å‹æ£€æŸ¥ï¼Œè¿è¡Œæ—¶æ‰å‘ç°é”™è¯¯
- âŒ IDE æ™ºèƒ½æç¤ºæ•ˆæœå·®
- âŒ ç”Ÿäº§ç¯å¢ƒè°ƒè¯•å›°éš¾

### 1.2 LangChain 1.x çš„ä¸‰å¤§é©å‘½æ€§æ”¹è¿›

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒLangChain å›¢é˜Ÿåœ¨ 2024 å¹´ 9 æœˆæ¨å‡ºäº† 1.0 æ­£å¼ç‰ˆï¼Œè¿›è¡Œäº†**æ¶æ„çº§é‡æ„**ã€‚

#### **æ”¹è¿›1ï¼šæ¨¡å—åŒ–åŒ…ç»“æ„ - æŒ‰éœ€å®‰è£…**

```python
# 1.x æ—¶ä»£ï¼šæ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹åŒ…
pip install langchain-core      # æ ¸å¿ƒæŠ½è±¡ï¼ˆ10MBï¼‰
pip install langchain-openai    # OpenAI é›†æˆï¼ˆ2MBï¼‰
pip install langchain-chroma    # Chroma é›†æˆï¼ˆ5MBï¼‰
# åªå®‰è£…éœ€è¦çš„éƒ¨åˆ†ï¼Œæ€»ä½“ç§¯ä» 500MB é™åˆ° 20MB
```

**åŒ…ç»“æ„å¯¹æ¯”**ï¼š

| 0.x åŒ…ç»“æ„ | 1.x åŒ…ç»“æ„ | ä½œç”¨ |
|-----------|-----------|------|
| `langchain` (å¤§æ‚çƒ©) | `langchain-core` | æ ¸å¿ƒæŠ½è±¡ç±»ï¼ˆBaseMessageã€Runnable ç­‰ï¼‰ |
| - | `langchain-openai` | OpenAI é›†æˆï¼ˆChatOpenAIã€Embeddingsï¼‰ |
| - | `langchain-community` | ç¤¾åŒºè´¡çŒ®çš„é›†æˆï¼ˆ200+ å·¥å…·ï¼‰ |
| - | `langchain-chroma` | Chroma å‘é‡æ•°æ®åº“é›†æˆ |
| - | `langchain-text-splitters` | æ–‡æœ¬åˆ†å‰²å·¥å…· |

**ä¼˜åŠ¿**ï¼š
- âœ… æŒ‰éœ€å®‰è£…ï¼Œéƒ¨ç½²ä½“ç§¯å‡å°‘ 95%
- âœ… é¿å…ä¾èµ–å†²çª
- âœ… æ›´æ–°æŸä¸ªé›†æˆä¸å½±å“å…¶ä»–éƒ¨åˆ†

#### **æ”¹è¿›2ï¼šç»Ÿä¸€çš„ `invoke()` è°ƒç”¨æ¥å£**

```python
# 1.x æ—¶ä»£ï¼šæ‰€æœ‰ç»„ä»¶ç»Ÿä¸€ä½¿ç”¨ invoke()
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o-mini")

# ç»Ÿä¸€è°ƒç”¨æ–¹å¼
result = llm.invoke([HumanMessage(content="ä½ å¥½")])
# è¿”å›ç»Ÿä¸€çš„ AIMessage å¯¹è±¡
```

**å¯¹æ¯”è¡¨æ ¼**ï¼š

| è°ƒç”¨æ–¹å¼ | 0.x | 1.x |
|---------|-----|-----|
| **æ–¹æ³•å** | `__call__` / `predict` / `run` | **ç»Ÿä¸€ä¸º `invoke()`** |
| **è¾“å…¥æ ¼å¼** | å­—ç¬¦ä¸² / æ¶ˆæ¯åˆ—è¡¨ | **ç»Ÿä¸€ä¸ºæ¶ˆæ¯åˆ—è¡¨** |
| **è¿”å›æ ¼å¼** | å­—ç¬¦ä¸² / å¯¹è±¡ | **ç»Ÿä¸€ä¸ºæ¶ˆæ¯å¯¹è±¡** |
| **ç±»å‹å®‰å…¨** | âŒ æ—  | âœ… å®Œæ•´ç±»å‹æ³¨è§£ |

**ä¼˜åŠ¿**ï¼š
- âœ… å­¦ä¹ æˆæœ¬é™ä½ 60%ï¼ˆåªéœ€è®°ä½ä¸€ä¸ªæ–¹æ³•ï¼‰
- âœ… æ¨¡å‹åˆ‡æ¢é›¶ä»£ç æ”¹åŠ¨
- âœ… IDE æ™ºèƒ½æç¤ºå®Œå–„

#### **æ”¹è¿›3ï¼šåŸºäº Pydantic v2 çš„ç±»å‹å®‰å…¨**

```python
# 1.x æ—¶ä»£ï¼šä¸¥æ ¼çš„ç±»å‹æ£€æŸ¥
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from typing import List

def process_messages(messages: List[BaseMessage]) -> AIMessage:
    llm = ChatOpenAI()
    response = llm.invoke(messages)  # IDE çŸ¥é“è¿”å› AIMessage
    return response  # ç±»å‹æ£€æŸ¥é€šè¿‡
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç¼–è¯‘æ—¶å‘ç° 90% çš„ç±»å‹é”™è¯¯
- âœ… IDE è‡ªåŠ¨è¡¥å…¨ç²¾å‡†
- âœ… ä»£ç å¯ç»´æŠ¤æ€§å¤§å¹…æå‡

---

## äºŒã€LangChain 1.x æ ¸å¿ƒæ¦‚å¿µå®æˆ˜

### 2.1 æ ¸å¿ƒæ¦‚å¿µæ¶æ„å›¾

```mermaid
graph TD
    A[LangChain 1.x æ ¸å¿ƒæ¶æ„] --> B[langchain-core]
    A --> C[langchain-openai]
    A --> D[langchain-community]

    B --> E[BaseMessage<br/>æ¶ˆæ¯åŸºç±»]
    B --> F[Runnable<br/>å¯è¿è¡Œæ¥å£]
    B --> G[BaseChatModel<br/>èŠå¤©æ¨¡å‹åŸºç±»]

    E --> E1[HumanMessage<br/>ç”¨æˆ·æ¶ˆæ¯]
    E --> E2[AIMessage<br/>AIå›å¤]
    E --> E3[SystemMessage<br/>ç³»ç»Ÿæç¤º]
    E --> E4[ToolMessage<br/>å·¥å…·ç»“æœ]

    C --> H[ChatOpenAI<br/>å®ç°ç±»]
    C --> I[OpenAIEmbeddings<br/>åµŒå…¥æ¨¡å‹]

    H --> J[invokeæ–¹æ³•]
    J --> K[ç»Ÿä¸€è°ƒç”¨å…¥å£]

    style B fill:#e1f5ff
    style C fill:#fff4e1
    style E fill:#e7f9e7
    style J fill:#ffe7e7
```

### 2.2 æ¶ˆæ¯ä½“ç³»ï¼ˆMessagesï¼‰- AI å¯¹è¯çš„æ ‡å‡†æ ¼å¼

åœ¨ LangChain 1.x ä¸­ï¼Œæ‰€æœ‰å¯¹è¯éƒ½ä½¿ç”¨**æ¶ˆæ¯å¯¹è±¡**è¿›è¡Œäº¤äº’ï¼Œè€Œä¸æ˜¯ç®€å•çš„å­—ç¬¦ä¸²ã€‚

#### **æ¶ˆæ¯ç±»å‹è¯¦è§£**

```python
from langchain_core.messages import (
    HumanMessage,    # ç”¨æˆ·æ¶ˆæ¯
    AIMessage,       # AI å›å¤
    SystemMessage,   # ç³»ç»Ÿæç¤ºï¼ˆè®¾å®š AI è§’è‰²ï¼‰
    ToolMessage      # å·¥å…·è°ƒç”¨ç»“æœ
)

# 1. HumanMessage - ç”¨æˆ·è¾“å…¥
user_msg = HumanMessage(content="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")

# 2. AIMessage - AI å›å¤
ai_msg = AIMessage(content="ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ 25Â°C")

# 3. SystemMessage - ç³»ç»Ÿæç¤ºï¼ˆå‘Šè¯‰ AI å®ƒçš„è§’è‰²ï¼‰
system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”æ’­æŠ¥å‘˜")

# 4. ToolMessage - å·¥å…·è°ƒç”¨ç»“æœï¼ˆåç»­ç« èŠ‚è¯¦è§£ï¼‰
tool_msg = ToolMessage(
    content='{"temperature": 25, "condition": "sunny"}',
    tool_call_id="call_123"
)
```

**ä¸ºä»€ä¹ˆéœ€è¦æ¶ˆæ¯å¯¹è±¡ï¼Ÿ**

å¯¹æ¯”ä¼ ç»Ÿå­—ç¬¦ä¸²æ–¹å¼ï¼š

```python
# âŒ 0.x æ—¶ä»£ï¼šå­—ç¬¦ä¸²æ‹¼æ¥ï¼Œå®¹æ˜“å‡ºé”™
prompt = f"ç³»ç»Ÿï¼šä½ æ˜¯å¤©æ°”æ’­æŠ¥å‘˜\nç”¨æˆ·ï¼š{user_input}\nAIï¼š"
response = llm(prompt)  # è¿”å›å­—ç¬¦ä¸²ï¼Œéš¾ä»¥è§£æ

# âœ… 1.x æ—¶ä»£ï¼šç»“æ„åŒ–æ¶ˆæ¯ï¼Œç±»å‹å®‰å…¨
messages = [
    SystemMessage(content="ä½ æ˜¯å¤©æ°”æ’­æŠ¥å‘˜"),
    HumanMessage(content=user_input)
]
response = llm.invoke(messages)  # è¿”å› AIMessage å¯¹è±¡
print(response.content)  # è®¿é—®å†…å®¹
print(response.response_metadata)  # è®¿é—®å…ƒæ•°æ®ï¼ˆtoken æ•°ç­‰ï¼‰
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»“æ„åŒ–æ•°æ®ï¼Œæ˜“äºè§£æå’Œå¤„ç†
- âœ… ä¿ç•™å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
- âœ… æ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ + å·¥å…·è°ƒç”¨ï¼‰

### 2.3 ChatOpenAI - ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯

#### **åŸºæœ¬ä½¿ç”¨**

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
llm = ChatOpenAI(
    model="gpt-4o-mini",          # æ¨¡å‹åç§°
    temperature=0.7,              # æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
    max_tokens=2000,              # æœ€å¤§ç”Ÿæˆ token æ•°
    api_key="your-api-key",       # API å¯†é’¥
    base_url="https://api.openai.com/v1"  # API åŸºç¡€ URL
)

# 2. è°ƒç”¨æ¨¡å‹
messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")]
response = llm.invoke(messages)

# 3. è·å–ç»“æœ
print(response.content)  # AI çš„å›å¤æ–‡æœ¬
print(response.response_metadata)  # å…ƒæ•°æ®ï¼ˆtoken ä½¿ç”¨é‡ç­‰ï¼‰
```

#### **å‚æ•°è¯¦è§£**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model` | str | "gpt-3.5-turbo" | æ¨¡å‹åç§°ï¼ˆgpt-4oã€gpt-4o-mini ç­‰ï¼‰ |
| `temperature` | float | 0.7 | æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0=ç¡®å®šæ€§ï¼Œ2=é«˜éšæœºï¼‰ |
| `max_tokens` | int | None | æœ€å¤§ç”Ÿæˆ token æ•° |
| `api_key` | str | ç¯å¢ƒå˜é‡ | OpenAI API å¯†é’¥ |
| `base_url` | str | OpenAI å®˜æ–¹ | API åŸºç¡€ URLï¼ˆå¯ç”¨äºå…¼å®¹æ¥å£ï¼‰ |
| `streaming` | bool | False | æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º |

#### **OpenAI å…¼å®¹æ¥å£çš„å¦™ç”¨**

LangChain 1.x çš„ `ChatOpenAI` æ”¯æŒæ‰€æœ‰ **OpenAI å…¼å®¹æ¥å£**ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥æ— ç¼åˆ‡æ¢åˆ°å…¶ä»–æœåŠ¡å•†ï¼š

```python
# ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ï¼ˆOpenAI å…¼å®¹ï¼‰
llm_aliyun = ChatOpenAI(
    model="qwen-plus",
    api_key="your-aliyun-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ä½¿ç”¨æœ¬åœ° Ollama
llm_local = ChatOpenAI(
    model="qwen:7b",
    api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå® key
    base_url="http://localhost:11434/v1"
)

# è°ƒç”¨æ–¹å¼å®Œå…¨ä¸€è‡´ï¼
response1 = llm_aliyun.invoke([HumanMessage(content="ä½ å¥½")])
response2 = llm_local.invoke([HumanMessage(content="ä½ å¥½")])
```

**ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ**
- âœ… å¼€å‘æ—¶ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ã€å¿«é€Ÿï¼‰
- âœ… ç”Ÿäº§æ—¶åˆ‡æ¢åˆ°äº‘ç«¯ï¼ˆç¨³å®šã€é«˜æ€§èƒ½ï¼‰
- âœ… ä»£ç é›¶æ”¹åŠ¨ï¼Œåªéœ€ä¿®æ”¹é…ç½®

### 2.4 invoke() ç»Ÿä¸€è°ƒç”¨æ¨¡å¼

#### **Runnable æ¥å£**

LangChain 1.x å¼•å…¥äº† `Runnable` æ¥å£ï¼Œæ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶ï¼ˆLLMã€Chainã€Toolï¼‰éƒ½å®ç°è¿™ä¸ªæ¥å£ï¼š

```python
from langchain_core.runnables import Runnable

# æ‰€æœ‰è¿™äº›ç»„ä»¶éƒ½æ˜¯ Runnable
llm: Runnable           # ChatOpenAI å®ç°äº† Runnable
chain: Runnable         # LLMChain å®ç°äº† Runnable
agent: Runnable         # Agent å®ç°äº† Runnable
```

**ç»Ÿä¸€çš„è°ƒç”¨æ–¹æ³•**ï¼š

```python
# æ‰€æœ‰ Runnable éƒ½æ”¯æŒè¿™äº›æ–¹æ³•
result = runnable.invoke(input)         # åŒæ­¥è°ƒç”¨
result = await runnable.ainvoke(input)  # å¼‚æ­¥è°ƒç”¨
for chunk in runnable.stream(input):    # æµå¼è°ƒç”¨
    print(chunk)
results = runnable.batch([input1, input2])  # æ‰¹é‡è°ƒç”¨
```

#### **å®æˆ˜å¯¹æ¯”ï¼š0.x vs 1.x**

**åœºæ™¯ï¼šè°ƒç”¨ LLM ç”Ÿæˆå›å¤**

```python
# ========== 0.x æ—¶ä»£ ==========
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI()
# æ–¹æ³•1ï¼šä½¿ç”¨ __call__
result1 = llm([HumanMessage(content="ä½ å¥½")])
# æ–¹æ³•2ï¼šä½¿ç”¨ predict
result2 = llm.predict("ä½ å¥½")
# æ–¹æ³•3ï¼šä½¿ç”¨ predict_messages
result3 = llm.predict_messages([HumanMessage(content="ä½ å¥½")])
# âŒ ä¸‰ç§æ–¹æ³•ï¼Œå®¹æ˜“æ··æ·†

# ========== 1.x æ—¶ä»£ ==========
from langchain_openai import ChatOpenAI  # æ³¨æ„æ–°çš„å¯¼å…¥è·¯å¾„
from langchain_core.messages import HumanMessage

llm = ChatOpenAI()
result = llm.invoke([HumanMessage(content="ä½ å¥½")])  # âœ… å”¯ä¸€æ–¹æ³•
```

---

## ä¸‰ã€5åˆ†é’Ÿå¿«é€Ÿå®æˆ˜ï¼šç¬¬ä¸€ä¸ª LangChain 1.x å¯¹è¯åº”ç”¨

### 3.1 ç¯å¢ƒå‡†å¤‡

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 2. å®‰è£…ä¾èµ–ï¼ˆæ³¨æ„æ–°çš„åŒ…åï¼‰
pip install langchain-core langchain-openai python-dotenv
```

### 3.2 é…ç½® API å¯†é’¥

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# .env æ–‡ä»¶å†…å®¹
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1  # å¯é€‰ï¼Œé»˜è®¤å€¼
```

### 3.3 å®Œæ•´ä»£ç å®ç°

åˆ›å»º `simple_chat.py` æ–‡ä»¶ï¼š

```python
"""
LangChain 1.x ç®€å•å¯¹è¯åº”ç”¨
åŠŸèƒ½ï¼šä¸ AI è¿›è¡Œå•è½®å¯¹è¯
"""
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
llm = ChatOpenAI(
    model="gpt-4o-mini",      # ä½¿ç”¨ mini ç‰ˆæœ¬ï¼Œæˆæœ¬æ›´ä½
    temperature=0.7,          # æ¸©åº¦å‚æ•°
    api_key=os.getenv("OPENAI_API_KEY"),  # ä»ç¯å¢ƒå˜é‡è¯»å–
    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
)

# 3. æ„é€ æ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·å­¦ä¹  LangChain æ¡†æ¶ã€‚"),
    HumanMessage(content="è¯·ç”¨ä¸€å¥è¯è§£é‡Š LangChain 1.x çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ")
]

# 4. è°ƒç”¨ LLM
print("æ­£åœ¨è°ƒç”¨ LLM...")
response = llm.invoke(messages)

# 5. è¾“å‡ºç»“æœ
print("\n=== AI å›å¤ ===")
print(response.content)

# 6. æŸ¥çœ‹å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
print("\n=== å…ƒæ•°æ® ===")
print(f"Token ä½¿ç”¨é‡: {response.response_metadata.get('token_usage', {})}")
print(f"æ¨¡å‹: {response.response_metadata.get('model_name', 'unknown')}")
```

### 3.4 è¿è¡Œæµ‹è¯•

```bash
python simple_chat.py
```

**é¢„æœŸè¾“å‡º**ï¼š

```
æ­£åœ¨è°ƒç”¨ LLM...

=== AI å›å¤ ===
LangChain 1.x çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯æ¨¡å—åŒ–æ¶æ„ã€ç»Ÿä¸€çš„ API æ¥å£å’Œå®Œå–„çš„ç±»å‹å®‰å…¨ï¼Œè®© AI åº”ç”¨å¼€å‘æ›´åŠ é«˜æ•ˆå’Œç¨³å®šã€‚

=== å…ƒæ•°æ® ===
Token ä½¿ç”¨é‡: {'prompt_tokens': 45, 'completion_tokens': 38, 'total_tokens': 83}
æ¨¡å‹: gpt-4o-mini
```

### 3.5 ä»£ç é€è¡Œè§£æ

```python
# ç¬¬1éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡
from dotenv import load_dotenv  # åŠ è½½ .env æ–‡ä»¶
load_dotenv()  # å°† .env ä¸­çš„å˜é‡åŠ è½½åˆ°ç¯å¢ƒå˜é‡

# ç¬¬2éƒ¨åˆ†ï¼šåˆå§‹åŒ– LLM
llm = ChatOpenAI(...)
# ChatOpenAI æ˜¯ langchain-openai åŒ…æä¾›çš„ç±»
# å®ç°äº† Runnable æ¥å£ï¼Œæ”¯æŒ invoke() æ–¹æ³•

# ç¬¬3éƒ¨åˆ†ï¼šæ„é€ æ¶ˆæ¯
messages = [SystemMessage(...), HumanMessage(...)]
# SystemMessage: è®¾å®š AI è§’è‰²å’Œè¡Œä¸ºå‡†åˆ™
# HumanMessage: ç”¨æˆ·çš„å…·ä½“é—®é¢˜

# ç¬¬4éƒ¨åˆ†ï¼šè°ƒç”¨ LLM
response = llm.invoke(messages)
# invoke() æ˜¯ 1.x çš„ç»Ÿä¸€è°ƒç”¨æ–¹æ³•
# è¿”å› AIMessage å¯¹è±¡

# ç¬¬5éƒ¨åˆ†ï¼šè·å–ç»“æœ
response.content  # AI çš„å›å¤æ–‡æœ¬
response.response_metadata  # å…ƒæ•°æ®ï¼ˆtokenã€æ¨¡å‹ç­‰ï¼‰
```

---

## å››ã€è¿›é˜¶å®æˆ˜ï¼šå¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡ç®¡ç†

### 4.1 å¤šè½®å¯¹è¯å®ç°

```python
"""
å¤šè½®å¯¹è¯ç¤ºä¾‹
åŠŸèƒ½ï¼šä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒè¿ç»­æé—®
"""
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

llm = ChatOpenAI(model="gpt-4o-mini")

# åˆå§‹åŒ–å¯¹è¯å†å²
conversation_history = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ª Python ç¼–ç¨‹åŠ©æ‰‹")
]

def chat(user_input: str) -> str:
    """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤"""
    # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    conversation_history.append(HumanMessage(content=user_input))

    # 2. è°ƒç”¨ LLM
    response = llm.invoke(conversation_history)

    # 3. æ·»åŠ  AI å›å¤åˆ°å†å²
    conversation_history.append(response)

    return response.content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("AI:", chat("ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"))
    print("\nAI:", chat("ç»™æˆ‘ä¸€ä¸ªä¾‹å­"))  # èƒ½ç†è§£"å®ƒ"æŒ‡çš„æ˜¯åˆ—è¡¨æ¨å¯¼å¼
    print("\nAI:", chat("å®ƒçš„æ€§èƒ½å¦‚ä½•ï¼Ÿ"))  # ç»§ç»­ä¿æŒä¸Šä¸‹æ–‡
```

**è¿è¡Œç»“æœ**ï¼š

```
AI: åˆ—è¡¨æ¨å¯¼å¼æ˜¯ Python ä¸­ä¸€ç§ç®€æ´çš„åˆ›å»ºåˆ—è¡¨çš„è¯­æ³•...

AI: å½“ç„¶ï¼ä¾‹å¦‚ï¼š[x**2 for x in range(10)] åˆ›å»ºå‰10ä¸ªæ•°çš„å¹³æ–¹åˆ—è¡¨

AI: åˆ—è¡¨æ¨å¯¼å¼çš„æ€§èƒ½é€šå¸¸ä¼˜äºä¼ ç»Ÿçš„ for å¾ªç¯ï¼Œå› ä¸º...
```

### 4.2 æµå¼è¾“å‡ºï¼ˆå®æ—¶å“åº”ï¼‰

```python
"""
æµå¼è¾“å‡ºç¤ºä¾‹
åŠŸèƒ½ï¼šé€å­—è¾“å‡º AI å›å¤ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
"""
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True  # å¯ç”¨æµå¼è¾“å‡º
)

messages = [HumanMessage(content="ç”¨100å­—ä»‹ç» LangChain 1.x çš„ä¼˜åŠ¿")]

print("AI å›å¤ï¼ˆæµå¼ï¼‰ï¼š", end="", flush=True)
for chunk in llm.stream(messages):
    print(chunk.content, end="", flush=True)  # å®æ—¶è¾“å‡ºæ¯ä¸ªå­—
print()  # æ¢è¡Œ
```

**æ•ˆæœ**ï¼šAI çš„å›å¤ä¼šåƒæ‰“å­—ä¸€æ ·é€å­—å‡ºç°ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å…¨éƒ¨æ˜¾ç¤ºã€‚

---

## äº”ã€LangChain 0.x â†’ 1.x è¿ç§»æŒ‡å—

### 5.1 å¯¼å…¥è·¯å¾„å˜æ›´å¯¹ç…§è¡¨

| åŠŸèƒ½ | 0.x å¯¼å…¥ | 1.x å¯¼å…¥ |
|------|---------|---------|
| **ChatOpenAI** | `from langchain.chat_models import ChatOpenAI` | `from langchain_openai import ChatOpenAI` |
| **æ¶ˆæ¯ç±»å‹** | `from langchain.schema import HumanMessage` | `from langchain_core.messages import HumanMessage` |
| **Prompt æ¨¡æ¿** | `from langchain.prompts import ChatPromptTemplate` | `from langchain_core.prompts import ChatPromptTemplate` |
| **å‘é‡å­˜å‚¨** | `from langchain.vectorstores import Chroma` | `from langchain_chroma import Chroma` |
| **æ–‡æœ¬åˆ†å‰²** | `from langchain.text_splitter import ...` | `from langchain_text_splitters import ...` |

### 5.2 API è°ƒç”¨æ–¹å¼å˜æ›´

```python
# ========== 0.x ä»£ç  ==========
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI()
result = llm.predict("ä½ å¥½")  # ä½¿ç”¨ predict
# æˆ–
result = llm([HumanMessage(content="ä½ å¥½")])  # ä½¿ç”¨ __call__

# ========== 1.x ä»£ç  ==========
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI()
result = llm.invoke([HumanMessage(content="ä½ å¥½")])  # ç»Ÿä¸€ä½¿ç”¨ invoke
```

### 5.3 ä¾èµ–å®‰è£…å˜æ›´

```bash
# ========== 0.x ä¾èµ– ==========
pip install langchain openai

# ========== 1.x ä¾èµ– ==========
pip install langchain-core langchain-openai
# åªå®‰è£…éœ€è¦çš„éƒ¨åˆ†ï¼Œä½“ç§¯æ›´å°
```

### 5.4 å®Œæ•´è¿ç§»æ­¥éª¤

**æ­¥éª¤1ï¼šæ›´æ–°ä¾èµ–**

```bash
# å¸è½½æ—§ç‰ˆæœ¬
pip uninstall langchain

# å®‰è£…æ–°ç‰ˆæœ¬æ ¸å¿ƒåŒ…
pip install langchain-core

# æŒ‰éœ€å®‰è£…é›†æˆåŒ…
pip install langchain-openai      # OpenAI é›†æˆ
pip install langchain-chroma      # Chroma é›†æˆ
pip install langchain-community   # ç¤¾åŒºå·¥å…·
```

**æ­¥éª¤2ï¼šæ›´æ–°å¯¼å…¥è·¯å¾„**

ä½¿ç”¨æ‰¹é‡æ›¿æ¢å·¥å…·ï¼ˆå¦‚ VS Code çš„å…¨å±€æœç´¢æ›¿æ¢ï¼‰ï¼š

```python
# æ›¿æ¢è§„åˆ™ç¤ºä¾‹
from langchain.chat_models import ChatOpenAI
â†’ from langchain_openai import ChatOpenAI

from langchain.schema import HumanMessage
â†’ from langchain_core.messages import HumanMessage
```

**æ­¥éª¤3ï¼šç»Ÿä¸€è°ƒç”¨æ–¹æ³•**

```python
# å°†æ‰€æœ‰ predictã€__call__ æ›¿æ¢ä¸º invoke
llm.predict(text)  â†’ llm.invoke([HumanMessage(content=text)])
llm(messages)      â†’ llm.invoke(messages)
```

**æ­¥éª¤4ï¼šæµ‹è¯•éªŒè¯**

```bash
# è¿è¡Œæµ‹è¯•ç¡®ä¿è¿ç§»æˆåŠŸ
pytest tests/
```

---

## å…­ã€0.x vs 1.x å®Œæ•´å¯¹æ¯”æ€»ç»“

| å¯¹æ¯”ç»´åº¦ | LangChain 0.x | LangChain 1.x | ä¼˜åŠ¿ |
|---------|--------------|--------------|------|
| **åŒ…ç»“æ„** | å•ä¸€å¤§åŒ… `langchain` | æ¨¡å—åŒ–æ‹†åˆ†ï¼ˆcoreã€openaiã€communityï¼‰ | ä½“ç§¯å‡å°‘ 95% |
| **å®‰è£…ä½“ç§¯** | 500MB+ | 20MBï¼ˆæŒ‰éœ€ï¼‰ | éƒ¨ç½²æ›´å¿« |
| **è°ƒç”¨æ–¹æ³•** | `predict` / `__call__` / `run` | ç»Ÿä¸€ `invoke()` | å­¦ä¹ æˆæœ¬é™ä½ 60% |
| **ç±»å‹å®‰å…¨** | âŒ æ—  | âœ… Pydantic v2 | ç¼–è¯‘æ—¶å‘ç°é”™è¯¯ |
| **æ¨¡å‹åˆ‡æ¢** | éœ€ä¿®æ”¹ä»£ç  | åªéœ€æ”¹é…ç½® | ç»´æŠ¤æˆæœ¬é™ä½ |
| **æµå¼è¾“å‡º** | éƒ¨åˆ†æ”¯æŒ | å®Œæ•´æ”¯æŒ `.stream()` | ç”¨æˆ·ä½“éªŒæ›´å¥½ |
| **å¼‚æ­¥æ”¯æŒ** | ä¸å®Œå–„ | å®Œæ•´æ”¯æŒ `.ainvoke()` | é«˜å¹¶å‘åœºæ™¯ |
| **æ–‡æ¡£å®Œå–„åº¦** | â­â­â­ | â­â­â­â­â­ | ä¸Šæ‰‹æ›´å¿« |

---

## ä¸ƒã€æœ¬ç« æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

âœ… **æ¶æ„å‡çº§**ï¼š
- LangChain 1.x é‡‡ç”¨æ¨¡å—åŒ–åŒ…ç»“æ„ï¼Œå®‰è£…ä½“ç§¯å‡å°‘ 95%
- æ‹†åˆ†ä¸º `core`ã€`openai`ã€`community` ç­‰ç‹¬ç«‹åŒ…

âœ… **ç»Ÿä¸€ API**ï¼š
- æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€ä½¿ç”¨ `invoke()` æ–¹æ³•è°ƒç”¨
- æ¶ˆæ¯ä½“ç³»ï¼ˆHumanMessageã€AIMessageï¼‰æ ‡å‡†åŒ–
- æ”¯æŒæµå¼è¾“å‡ºï¼ˆ`.stream()`ï¼‰å’Œå¼‚æ­¥è°ƒç”¨ï¼ˆ`.ainvoke()`ï¼‰

âœ… **ç±»å‹å®‰å…¨**ï¼š
- åŸºäº Pydantic v2 å®ç°å®Œæ•´ç±»å‹æ£€æŸ¥
- IDE æ™ºèƒ½æç¤ºæ›´ç²¾å‡†
- ç¼–è¯‘æ—¶å‘ç° 90% çš„é”™è¯¯

âœ… **OpenAI å…¼å®¹**ï¼š
- `ChatOpenAI` æ”¯æŒæ‰€æœ‰ OpenAI å…¼å®¹æ¥å£
- å¯æ— ç¼åˆ‡æ¢ OpenAI / é˜¿é‡Œäº‘ / Ollama æœ¬åœ°æ¨¡å‹
- ä»£ç é›¶æ”¹åŠ¨ï¼Œåªéœ€ä¿®æ”¹é…ç½®

### å…³é”®ä»£ç æ¨¡æ¿

```python
# LangChain 1.x æ ‡å‡†è°ƒç”¨æ¨¡æ¿
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# 1. åˆå§‹åŒ–
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# 2. æ„é€ æ¶ˆæ¯
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    HumanMessage(content="ç”¨æˆ·é—®é¢˜")
]

# 3. è°ƒç”¨
response = llm.invoke(messages)

# 4. è·å–ç»“æœ
print(response.content)
```

---

## å…«ã€ä¸‹èŠ‚é¢„å‘Š

**ç¬¬02ç« ï¼šLangGraph 1.x å·¥ä½œæµç¼–æ’ - æ‰“é€ ä¼šæ€è€ƒçš„æ™ºèƒ½ Agent**

åœ¨ç¬¬02ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š

1. **ä¸ºä»€ä¹ˆéœ€è¦ LangGraph**ï¼šLangChain çš„é“¾å¼ç»“æ„æ— æ³•å¤„ç†å¤æ‚æ¨ç†ï¼ŒLangGraph çš„å›¾ç»“æ„å¦‚ä½•è§£å†³
2. **StateGraph æ ¸å¿ƒæ¦‚å¿µ**ï¼šèŠ‚ç‚¹ï¼ˆNodeï¼‰ã€è¾¹ï¼ˆEdgeï¼‰ã€æ¡ä»¶è·¯ç”±ï¼ˆConditional Edgeï¼‰
3. **ReAct Agent å®æˆ˜**ï¼šå®ç° Thought â†’ Action â†’ Observation æ¨ç†å¾ªç¯
4. **LangGraph 0.x vs 1.x**ï¼šAPI å˜æ›´ã€æ–°å¢ç‰¹æ€§è¯¦è§£
5. **é‡‘èå®¢æœåœºæ™¯**ï¼šä¸ºä»€ä¹ˆé‡‘èæ™ºèƒ½å®¢æœéœ€è¦ Agent èƒ½åŠ›

è®©æˆ‘ä»¬ç»§ç»­æ·±å…¥ LangGraph çš„ä¸–ç•Œï¼Œä¸ºåç»­çš„é‡‘èæ™ºèƒ½å®¢æœé¡¹ç›®æ‰“ä¸‹åšå®åŸºç¡€ï¼ğŸš€

---

**ç‰ˆæœ¬ä¿¡æ¯**ï¼š
- æ•™ç¨‹ç‰ˆæœ¬ï¼šv1.0
- LangChain ç‰ˆæœ¬ï¼š1.0.7+
- æœ€åæ›´æ–°ï¼š2025-01-16
